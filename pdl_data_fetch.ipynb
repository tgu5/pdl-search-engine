{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDtWgbStagYO",
        "outputId": "cc8c39d7-69f0-4a8d-e5c4-46d6d7625ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34906 available records in this search\n",
            "Retrieved 100 records in batch 1\n",
            "Successfully recovered 100 profiles in 2 batches [8.543031930923462 seconds]\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/python3\n",
        "import requests, json, time, csv\n",
        "\n",
        "# TODO: Add your api key below\n",
        "API_KEY = \"c67af6f09a40fd32d5f409439e05fef62ea8c75490d99fc5223eb3d5850f367b\"\n",
        "\n",
        "# TODO: Set number of records to pull (-1 for all available records)\n",
        "MAX_NUM_RECORDS = 100\n",
        "\n",
        "# NO CHANGES NEEDED BELOW HERE\n",
        "PDL_URL = \"https://api.peopledatalabs.com/v5/person/search\"\n",
        "request_header = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"X-api-key\": API_KEY\n",
        "}\n",
        "\n",
        "SQL_QUERY = \"SELECT * FROM person WHERE (job_title = 'software engineer' and location_region = 'new york')\"\n",
        "\n",
        "num_records_to_request = 10\n",
        "params = {\n",
        "    \"dataset\": \"all\",\n",
        "    \"sql\": SQL_QUERY,\n",
        "    \"size\": num_records_to_request,\n",
        "    \"pretty\": True\n",
        "}\n",
        "\n",
        "# Pull all results in multiple batches\n",
        "batch = 1\n",
        "all_records = []\n",
        "start_time = time.time()\n",
        "while batch == 1 or params[\"scroll_token\"]:\n",
        "    if MAX_NUM_RECORDS != -1:\n",
        "        # Update num_records_to_request\n",
        "        # Compute the number of records left to pull\n",
        "        num_records_to_request = MAX_NUM_RECORDS - len(all_records)\n",
        "        # Clamp this number between 0 and 100\n",
        "        num_records_to_request = max(0, min(num_records_to_request, 100))\n",
        "\n",
        "    if num_records_to_request == 0:\n",
        "        break\n",
        "\n",
        "    params[\"size\"] = num_records_to_request\n",
        "    response = requests.get(PDL_URL, headers=request_header, params=params).json()\n",
        "\n",
        "    if batch == 1:\n",
        "        print(f\"{response['total']} available records in this search\")\n",
        "\n",
        "    all_records.extend(response.get(\"data\", []))\n",
        "    params[\"scroll_token\"] = response.get(\"scroll_token\")\n",
        "    print(f\"Retrieved {len(response.get('data', []))} records in batch {batch}\")\n",
        "    batch += 1\n",
        "\n",
        "    if params[\"scroll_token\"]:\n",
        "        time.sleep(6)   # avoid hitting rate limit thresholds\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "\n",
        "print(f\"Successfully recovered {len(all_records)} profiles in \"\n",
        "      f\"{batch} batches [{runtime} seconds]\")\n",
        "\n",
        "\n",
        "def save_profiles_to_csv(profiles, filename, fields=[], delim=\",\"):\n",
        "    \"\"\"Save profiles to csv (utility function)\"\"\"\n",
        "\n",
        "    # Define header fields\n",
        "    if fields == [] and len(profiles) > 0:\n",
        "        fields = profiles[0].keys()\n",
        "\n",
        "    with open(filename, \"w\") as csvfile:\n",
        "        # Write csv file\n",
        "        writer = csv.writer(csvfile, delimiter=delim)\n",
        "\n",
        "        # Write Header:\n",
        "        writer.writerow(fields)\n",
        "\n",
        "        count = 0\n",
        "        for profile in profiles:\n",
        "            # Write Body:\n",
        "            writer.writerow([profile[field] for field in fields])\n",
        "            count += 1\n",
        "            print(f\"Wrote {count} lines to: '{filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "parsed_records = []\n",
        "for record in all_records:\n",
        "  parsed_records.append({\n",
        "      'name': record['full_name'],\n",
        "      'inferred_years_experience': record['inferred_years_experience'],\n",
        "      'companies_worked_at': [e['company']['name'] for e in record['experience']],\n",
        "      'skills': record['skills'],\n",
        "      'inferred_salary': str(random.randint(100, 300)) + 'k',\n",
        "      'recommended_personal_email': record['recommended_personal_email']\n",
        "      })"
      ],
      "metadata": {
        "id": "TefnsHymdC4h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "prisma_data = []\n",
        "for record in parsed_records:\n",
        "    prisma_record = {\n",
        "        \"name\": record['name'],\n",
        "        \"inferred_years_experience\": str(record['inferred_years_experience']),  # Convert to string\n",
        "        \"companies_worked_at\": json.dumps(record['companies_worked_at']),  # Convert to JSON string\n",
        "        \"skills\": json.dumps(record['skills']),  # Convert to JSON string\n",
        "        \"inferred_salary\": record['inferred_salary'],\n",
        "        \"recommended_personal_email\": 'n/a' if record['recommended_personal_email'] is None else record['recommended_personal_email'],\n",
        "    }\n",
        "    prisma_data.append(prisma_record)\n",
        "\n",
        "with open('prisma_seed.json', 'w') as f:\n",
        "    json.dump(prisma_data, f, indent=4)"
      ],
      "metadata": {
        "id": "LQ0K4F9xfBbh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_companies = []\n",
        "for record in parsed_records:\n",
        "  for company in record['companies_worked_at']:\n",
        "    if company not in all_companies:\n",
        "      all_companies.append(company)"
      ],
      "metadata": {
        "id": "KfY3yCd7hd4b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "457Mh25i2Fs7"
      }
    }
  ]
}